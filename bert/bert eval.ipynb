{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 06:58:55.383507 139828601800512 file_utils.py:39] PyTorch version 1.1.0 available.\n",
      "I1114 06:58:55.424413 139828601800512 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I1114 06:58:55.432470 139828601800512 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "import bert_model\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 06:59:36.286322 139828601800512 modeling.py:580] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/superceed1/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "I1114 06:59:36.288807 139828601800512 modeling.py:588] extracting archive file /home/superceed1/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmps8yabccx\n",
      "I1114 06:59:42.496191 139828601800512 modeling.py:598] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1114 06:59:44.585217 139828601800512 modeling.py:648] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I1114 06:59:44.585840 139828601800512 modeling.py:651] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "bert = bert_model.load_bert_model('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: TextList\n",
       "[CLS] grandma terri should burn in trash grandma terri is trash i hate grandma terri fk her to hell [SEP],[CLS] may utc it would be easiest if you were to admit to being a member of the involved portuguese lodge and then there would be no requirement to acknowledge whether you had a previous account carlos bot ##el ##ho did not have a good record or not and i would then remove the sock ##pu ##ppet template as irrelevant w ##pc ##oi permit is people to edit those articles such as ms ##ja ##pan does but just means you have to be more careful in ensuring that references back your edit is and that np ##ov is upheld [SEP],[CLS] the object ##ivity of this discussion is doubtful none ##xi ##sten ##t as indicated earlier the section on marxist leaders views is misleading a it lays un ##war ##rant ##ed and excessive emphasis on tr ##ots ##ky creating the misleading impression that other prominent marxist ##s marx eng ##els lenin did not advocate and ##or practiced terrorism b it lays un ##war ##rant ##ed and excessive emphasis on the theoretical rejection of individual terrorism creating the misleading impression that this is the main only marxist position on terrorism the discussion is not being properly monitored a no disc ##ern ##ible attempt is being made to establish and maintain an acceptable degree of object ##ivity b important and relevant scholarly works such as the international encyclopedia of terrorism are being ignored or illicit ##ly excluded from the discussion c though the only logical way to remedy the b ##lat ##ant im ##balance in the above section is to include quotes by ##on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with imp ##uni ##ty by the ap ##ologists for marxist terrorism who have done their best to sabotage and wreck both the article and the discussion among the tactics deployed by the ap ##ologist wreck ##ers and sa ##bot ##eurs the following may be identified as representative examples a it is claimed that marx and eng ##els did not advocate terrorism despite the fact that scholarly works like the international encyclopedia of terrorism [SEP],[CLS] shelly shock shelly shock is [SEP],[CLS] i do not care refer to on ##g ten ##g che ##ong talk page is la go ##ut ##te de pl ##ui ##e writing a biography or writing the history of trade unions she is making use of the dead to push her agenda again right before elections too how timely [SEP]\n",
       "y: MultiCategoryList\n",
       "toxic,,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: TextList\n",
       "[CLS] gee ##z are you forget ##ful we have already discussed why marx was not an anarchist ie he wanted to use a state to mold his socialist man er ##go he is a stat ##ist the opposite of an anarchist i know a guy who says that when he gets old and his teeth fall out hell quit eating meat would you call him a vegetarian [SEP],[CLS] car ##io ##ca rf ##a thanks for your support on my request for ad ##mins ##hip the final outcome was so i am now an administrator if you have any comments or concerns on my actions as an administrator please let me know thank you [SEP],[CLS] birthday no worries it is what i do enjoy ur day ##talk ##e [SEP],[CLS] pseudo ##sc ##ience category i am assuming that this article is in the pseudo ##sc ##ience category because of it is association with creation ##ism however there are modern scientific ##ally ##ac ##ce ##pt ##ed variants of cat ##ast ##rop ##hism that have nothing to do with creation ##ism and they are even mentioned in the article i think the connection to pseudo ##sc ##ience needs to be clarified or the article made more general and less creation ##isms ##pe ##ci ##fi ##c and the category tag removed entirely [SEP],[CLS] and if such phrase exists it would be provided by search engine even if mentioned page is not available as a whole [SEP]\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7f2bd08c5ae8>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Embedding(30522, 768, padding_idx=0)\n",
       "  (1): Embedding(512, 768)\n",
       "  (2): Embedding(2, 768)\n",
       "  (3): BertLayerNorm()\n",
       "  (4): Dropout(p=0.1)\n",
       "  (5): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (6): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (7): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (8): Dropout(p=0.1)\n",
       "  (9): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (10): BertLayerNorm()\n",
       "  (11): Dropout(p=0.1)\n",
       "  (12): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (13): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (14): BertLayerNorm()\n",
       "  (15): Dropout(p=0.1)\n",
       "  (16): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (17): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (18): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (19): Dropout(p=0.1)\n",
       "  (20): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (21): BertLayerNorm()\n",
       "  (22): Dropout(p=0.1)\n",
       "  (23): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (24): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (25): BertLayerNorm()\n",
       "  (26): Dropout(p=0.1)\n",
       "  (27): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (28): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (29): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (30): Dropout(p=0.1)\n",
       "  (31): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (32): BertLayerNorm()\n",
       "  (33): Dropout(p=0.1)\n",
       "  (34): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (35): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (36): BertLayerNorm()\n",
       "  (37): Dropout(p=0.1)\n",
       "  (38): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (39): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (40): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (41): Dropout(p=0.1)\n",
       "  (42): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (43): BertLayerNorm()\n",
       "  (44): Dropout(p=0.1)\n",
       "  (45): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (46): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (47): BertLayerNorm()\n",
       "  (48): Dropout(p=0.1)\n",
       "  (49): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (50): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (51): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (52): Dropout(p=0.1)\n",
       "  (53): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (54): BertLayerNorm()\n",
       "  (55): Dropout(p=0.1)\n",
       "  (56): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (57): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (58): BertLayerNorm()\n",
       "  (59): Dropout(p=0.1)\n",
       "  (60): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (61): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (62): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (63): Dropout(p=0.1)\n",
       "  (64): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (65): BertLayerNorm()\n",
       "  (66): Dropout(p=0.1)\n",
       "  (67): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (68): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (69): BertLayerNorm()\n",
       "  (70): Dropout(p=0.1)\n",
       "  (71): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (72): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (73): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (74): Dropout(p=0.1)\n",
       "  (75): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (76): BertLayerNorm()\n",
       "  (77): Dropout(p=0.1)\n",
       "  (78): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (79): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (80): BertLayerNorm()\n",
       "  (81): Dropout(p=0.1)\n",
       "  (82): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (83): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (84): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (85): Dropout(p=0.1)\n",
       "  (86): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (87): BertLayerNorm()\n",
       "  (88): Dropout(p=0.1)\n",
       "  (89): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (90): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (91): BertLayerNorm()\n",
       "  (92): Dropout(p=0.1)\n",
       "  (93): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (94): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (95): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (96): Dropout(p=0.1)\n",
       "  (97): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (98): BertLayerNorm()\n",
       "  (99): Dropout(p=0.1)\n",
       "  (100): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (101): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (102): BertLayerNorm()\n",
       "  (103): Dropout(p=0.1)\n",
       "  (104): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (105): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (106): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (107): Dropout(p=0.1)\n",
       "  (108): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (109): BertLayerNorm()\n",
       "  (110): Dropout(p=0.1)\n",
       "  (111): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (112): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (113): BertLayerNorm()\n",
       "  (114): Dropout(p=0.1)\n",
       "  (115): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (116): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (117): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (118): Dropout(p=0.1)\n",
       "  (119): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (120): BertLayerNorm()\n",
       "  (121): Dropout(p=0.1)\n",
       "  (122): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (123): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (124): BertLayerNorm()\n",
       "  (125): Dropout(p=0.1)\n",
       "  (126): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (127): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (128): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (129): Dropout(p=0.1)\n",
       "  (130): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (131): BertLayerNorm()\n",
       "  (132): Dropout(p=0.1)\n",
       "  (133): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (134): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (135): BertLayerNorm()\n",
       "  (136): Dropout(p=0.1)\n",
       "  (137): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (138): Tanh()\n",
       "  (139): Dropout(p=0.1)\n",
       "  (140): Linear(in_features=768, out_features=6, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.load('head-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MultiCategory ,\n",
       " tensor([0., 0., 0., 0., 0., 0.]),\n",
       " tensor([4.0619e-02, 2.0407e-05, 2.1858e-03, 4.0995e-04, 3.2507e-03, 1.0345e-03]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'you are so sweet'\n",
    "bert.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'you are pathetic piece of shit'\n",
    "result = bert.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.043986607, tensor(0.9807)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[5.6693e-04, 2.5819e-05, 6.7672e-05, 2.6106e-05, 9.2746e-05, 4.1498e-05],\n",
       "         [5.2298e-04, 3.0124e-05, 6.4323e-05, 2.8786e-05, 9.3707e-05, 4.4073e-05],\n",
       "         [4.5596e-04, 4.0818e-05, 5.9363e-05, 3.5280e-05, 9.7039e-05, 5.0652e-05],\n",
       "         ...,\n",
       "         [5.6003e-04, 2.6394e-05, 6.7182e-05, 2.6463e-05, 9.2870e-05, 4.1835e-05],\n",
       "         [6.3144e-04, 2.1654e-05, 7.2082e-05, 2.3548e-05, 9.2366e-05, 3.9185e-05],\n",
       "         [4.8418e-03, 1.0551e-05, 2.6650e-04, 4.8724e-05, 2.7955e-04, 1.0864e-04]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[CLS] i hope you are having a great holiday and at your convenience could you please get back to me i want to first cong ##rat ##ulate you on over contributions i know first hand the feeling you get when achieving great things you and your fellow administrators have made and continue to make wikipedia a brilliant place to go for correct information i want to apologize if the page</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] to paul b or whom ##ever has taken the responsibility for edit ##ting this work while i admire your dedication to uphold ##ing a neutral point of view in this article i must voice the opinion that your ##e lack ac ##k ##now ##led ##gm ##ent towards the fact that he is ##was a controversial figure to indians is not inline in general with wikipedia there are many examples</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] una ##uth ##oris ##ed change on january ie just in the last few weeks user ##m ##zi ##lika ##zi changed this article from the standard form of writing dates in the english language to ce ##bc ##e as he well knows this violate ##s w ##pm ##os specifically w ##per ##a which requires that any change to an articles date style first be discussed on the talk page and</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] am ##bari ##sh sri ##vas ##ta ##va am ##bari ##sh sri ##vas ##ta ##va good evening sir above article am ##bari ##sh sri ##vas ##ta ##va was nominated for del ##eti ##on review on february the result of the discussion was moved to main space you have participated in that discussion it was thoroughly checked and edited by you also i am grateful to you for your kind support</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] advice to save me writing too much a lot of this is going to be links to existing policies and guidelines there is plenty of advice available in fact a confusing amount so pointer ##s may be helpful the guide to writing your first article and the business fa ##q are particularly useful wikipedia frowns on writing about your own organisation because of worries that someone with a conflict</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bert.pred_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.6693e-04, 2.5819e-05, 6.7672e-05, 2.6106e-05, 9.2746e-05, 4.1498e-05],\n",
       "        [5.2298e-04, 3.0124e-05, 6.4323e-05, 2.8786e-05, 9.3707e-05, 4.4073e-05],\n",
       "        [4.5596e-04, 4.0818e-05, 5.9363e-05, 3.5280e-05, 9.7039e-05, 5.0652e-05],\n",
       "        [4.7330e-04, 3.7391e-05, 6.0560e-05, 3.3231e-05, 9.5877e-05, 4.8539e-05],\n",
       "        [4.7872e-04, 3.6436e-05, 6.0949e-05, 3.2659e-05, 9.5576e-05, 4.7952e-05],\n",
       "        [4.6359e-04, 3.9224e-05, 5.9886e-05, 3.4317e-05, 9.6471e-05, 4.9663e-05],\n",
       "        [1.1613e-01, 5.2986e-05, 7.0334e-03, 1.2046e-03, 1.1638e-02, 2.8328e-03],\n",
       "        [8.4817e-04, 1.5383e-05, 8.6428e-05, 2.0202e-05, 9.6748e-05, 3.7431e-05],\n",
       "        [5.7027e-04, 2.5551e-05, 6.7912e-05, 2.5940e-05, 9.2699e-05, 4.1342e-05],\n",
       "        [5.0186e-04, 3.2831e-05, 6.2683e-05, 3.0461e-05, 9.4470e-05, 4.5729e-05]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\", \"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../data/toxic_comment/cleaned/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_pred = bert.get_preds(list(test_df['comment_text'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[4.7520e-04, 3.7057e-05, 6.0700e-05, 3.3038e-05, 9.5794e-05, 4.8338e-05],\n",
       "         [4.8894e-04, 3.4752e-05, 6.1703e-05, 3.1639e-05, 9.5052e-05, 4.6914e-05],\n",
       "         [4.9875e-04, 3.3275e-05, 6.2438e-05, 3.0734e-05, 9.4591e-05, 4.6002e-05],\n",
       "         ...,\n",
       "         [3.8317e-03, 1.0634e-05, 2.2873e-04, 4.0591e-05, 2.2862e-04, 8.9445e-05],\n",
       "         [1.6366e-03, 1.1263e-05, 1.3264e-04, 2.2775e-05, 1.2899e-04, 4.8231e-05],\n",
       "         [6.0834e-04, 2.2929e-05, 7.0527e-05, 2.4324e-05, 9.2362e-05, 3.9859e-05]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = list(test_df['comment_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = bert.predict(test_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame([], columns=label_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000051</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000048</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.430046</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.043833</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.013767</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000038</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000050</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000043</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000051</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000039</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000052</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.998115</td>\n",
       "      <td>0.394998</td>\n",
       "      <td>0.991098</td>\n",
       "      <td>0.034199</td>\n",
       "      <td>0.932204</td>\n",
       "      <td>0.171291</td>\n",
       "      <td>toxic;obscene;insult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      toxic  severe_toxic   obscene    threat    insult  identity_hate  \\\n",
       "0  0.000451      0.000042  0.000059  0.000036  0.000097       0.000051   \n",
       "1  0.001625      0.000011  0.000132  0.000023  0.000129       0.000048   \n",
       "2  0.430046      0.000578  0.043833  0.006330  0.076600       0.013767   \n",
       "3  0.000724      0.000018  0.000078  0.000021  0.000093       0.000038   \n",
       "4  0.000465      0.000039  0.000060  0.000034  0.000096       0.000050   \n",
       "5  0.001323      0.000012  0.000116  0.000021  0.000115       0.000043   \n",
       "6  0.000456      0.000041  0.000059  0.000035  0.000097       0.000051   \n",
       "7  0.001007      0.000014  0.000097  0.000020  0.000102       0.000039   \n",
       "8  0.000447      0.000043  0.000059  0.000037  0.000098       0.000052   \n",
       "9  0.998115      0.394998  0.991098  0.034199  0.932204       0.171291   \n",
       "\n",
       "               category  \n",
       "0                        \n",
       "1                        \n",
       "2                        \n",
       "3                        \n",
       "4                        \n",
       "5                        \n",
       "6                        \n",
       "7                        \n",
       "8                        \n",
       "9  toxic;obscene;insult  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_write_values = []\n",
    "for value in test_values[:10]:\n",
    "    result = bert.predict(value)\n",
    "    percentage = result[2]\n",
    "    predicted_category = result[0]\n",
    "    \n",
    "    to_write = percentage.tolist()\n",
    "    to_write.append(predicted_category)\n",
    "    submission_df.loc[-1] = to_write\n",
    "    submission_df.index = submission_df.index + 1  # shifting index\n",
    "    submission_df = submission_df.sort_index() \n",
    "#     break\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submit = test_df[['id', 'comment_text']].copy()\n",
    "Submit = Submit[:10]\n",
    "Submit = pd.concat([Submit,submission_df],axis=1)\n",
    "Submit.to_csv(\"../output/bert.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
