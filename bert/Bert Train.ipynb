{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pretrainedmodels in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (0.7.4)\n",
      "Requirement already satisfied: munch in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pretrainedmodels) (2.5.0)\n",
      "Requirement already satisfied: torchvision in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pretrainedmodels) (0.3.0)\n",
      "Requirement already satisfied: tqdm in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pretrainedmodels) (4.29.1)\n",
      "Requirement already satisfied: torch in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pretrainedmodels) (1.3.0)\n",
      "Requirement already satisfied: six in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from munch->pretrainedmodels) (1.12.0)\n",
      "Requirement already satisfied: numpy in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from torchvision->pretrainedmodels) (1.16.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from torchvision->pretrainedmodels) (6.1.0)\n",
      "Requirement already satisfied: pytorch-pretrained-bert in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (0.6.2)\n",
      "Requirement already satisfied: torch>=0.4.1 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.16.5)\n",
      "Requirement already satisfied: requests in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.22.0)\n",
      "Requirement already satisfied: tqdm in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.29.1)\n",
      "Requirement already satisfied: regex in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2019.11.1)\n",
      "Requirement already satisfied: boto3 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.10.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.25.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.8 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.13.8)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.8->boto3->pytorch-pretrained-bert) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.8->boto3->pytorch-pretrained-bert) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.8->boto3->pytorch-pretrained-bert) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pretrainedmodels\n",
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fast ai \n",
    "import fastai\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.text import *\n",
    "\n",
    "from torchvision.models import *\n",
    "import pretrainedmodels\n",
    "\n",
    "import sys\n",
    "\n",
    "from fastai.callbacks.tracker import EarlyStoppingCallback\n",
    "from fastai.callbacks.tracker import SaveModelCallback\n",
    "from fastai.utils import *\n",
    "from torch.utils import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 00:04:50.683957 139984517506880 file_utils.py:39] PyTorch version 1.1.0 available.\n",
      "I1113 00:04:50.706103 139984517506880 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I1113 00:04:51.852593 139984517506880 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/superceed1/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "bert_tok = BertTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastAiBertTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n",
    "        self._pretrained_tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "    \n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '../data/toxic_comment/cleaned/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_root + 'train.csv')\n",
    "test_df = pd.read_csv(data_root + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 8:2 ratio\n",
    "train_df.fillna('no comment', inplace=True)\n",
    "test_df.fillna('no comment', inplace=True)\n",
    "train, val = train_test_split(\n",
    "    train_df, shuffle=True, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127656, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap BERT VOCAB and BERT tokenizer with fast ai modules\n",
    "fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_tokenizer = Tokenizer(\n",
    "    tok_func=FastAiBertTokenizer(\n",
    "        bert_tok, max_seq_len=256\n",
    "    ), pre_rules=[], post_rules=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create the databunch\n",
    "# Note bert tokenizer and bert vocab used here\n",
    "# Fill in some missing values here\n",
    "label_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "data_bunch_train = TextClasDataBunch.from_df(\n",
    "    \".\", train, val,\n",
    "    tokenizer=fastai_tokenizer,\n",
    "    vocab=fastai_bert_vocab,\n",
    "    include_bos=False,\n",
    "    include_eos=False,\n",
    "    text_cols=\"comment_text\",\n",
    "    label_cols=label_columns,\n",
    "    bs=12,\n",
    "    collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[CLS] words of encouragement to user dame ##m ##k in the cases of dispute with the users engaged in bulgarian nationalist ##ic propaganda dame ##m ##k you have come to the same conclusions about bulgarian th century nationalist ##ic propaganda on english wikipedia user ##lav ##eo ##l is one of the most prominent editors on wikipedia contributing false information in the articles about republic of macedonia macedonian nation ##his ##tory</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] hungarian and some british sources from the late th and early th centuries used the spelling rum ##ania with a u to describe romania the purpose of this spelling was to erase the link ##age that the romanian national movement was trying to install with the da ##cor ##oman ##s the ancient people who came to be after the roman invasion to modern romania no one except perhaps to</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] background ##color f ##df ##fe rows ##pan val ##ign ##top rows ##pan style ##fo ##nts ##ize xl ##ar ##ge pad ##ding vertical ##ali ##gn bottom height em football soccer barns ##tar style ##vert ##ical ##ali ##gn top border ##top p ##x solid gray for the great job you ##ve done in improving celtic fc season taking it from a ones ##ente ##nce stu ##b to a respectable article thanks</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] this is a very inaccurate and well poisoning characterization i am biologist with a strong background in philosophy mainly focused on ethics but of course broadly on meta ##physics ep ##iste ##mology and political philosophy as well at this point in my life i have gone back to school specifically to get a degree in philosophy and no longer even work as a scientist so for you to cast</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] images _ via ##cy ##rix _ iii ##m ##h ##z _ x _ v ##j ##pg listed for del ##eti ##on dear up ##load ##er the media file you uploaded as images _ via ##cy ##rix _ iii ##m ##h ##z _ x _ v ##j ##pg has been listed for speedy del ##eti ##on because you selected a copyright license type implying some type of restricted use such</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_bunch_train.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 00:05:38.087436 139984517506880 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained bert model from pytorch pretrained bert\n",
    "from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification, BertForNextSentencePrediction, BertForMaskedLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 00:05:39.371530 139984517506880 modeling.py:580] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/superceed1/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "I1113 00:05:39.375857 139984517506880 modeling.py:588] extracting archive file /home/superceed1/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpezi71ezl\n",
      "I1113 00:05:42.105474 139984517506880 modeling.py:598] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1113 00:05:44.017016 139984517506880 modeling.py:648] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I1113 00:05:44.017634 139984517506880 modeling.py:651] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "bert_model_class = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', num_labels=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_02 = partial(accuracy_thresh, thresh=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert_model_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learner function\n",
    "from fastai.callback import *\n",
    "\n",
    "learner = Learner(\n",
    "    data_bunch_train, model,\n",
    "    loss_func=loss_func, model_dir='model/', metrics=acc_02\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code will help us in splitting the model into desirable parts which will be helpful for us in Discriminative Learning i.e. setting up different learning rates and weight decays for different parts of the model.\n",
    "def bert_class_split(self) -> List[nn.Module]:\n",
    "    bert = model.bert\n",
    "    embedder = bert.embeddings\n",
    "    pooler = bert.pooler\n",
    "    encoder = bert.encoder\n",
    "    classifier = [model.dropout, model.classifier]\n",
    "    n = len(encoder.layer) // 3\n",
    "    print(n)\n",
    "    groups = [[embedder], list(encoder.layer[:n]), list(encoder.layer[n+1:2*n]), list(encoder.layer[(2*n)+1:]), [pooler], classifier]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "x = bert_class_split(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: TextList\n",
       "[CLS] grandma terri should burn in trash grandma terri is trash i hate grandma terri fk her to hell [SEP],[CLS] may utc it would be easiest if you were to admit to being a member of the involved portuguese lodge and then there would be no requirement to acknowledge whether you had a previous account carlos bot ##el ##ho did not have a good record or not and i would then remove the sock ##pu ##ppet template as irrelevant w ##pc ##oi permit is people to edit those articles such as ms ##ja ##pan does but just means you have to be more careful in ensuring that references back your edit is and that np ##ov is upheld [SEP],[CLS] the object ##ivity of this discussion is doubtful none ##xi ##sten ##t as indicated earlier the section on marxist leaders views is misleading a it lays un ##war ##rant ##ed and excessive emphasis on tr ##ots ##ky creating the misleading impression that other prominent marxist ##s marx eng ##els lenin did not advocate and ##or practiced terrorism b it lays un ##war ##rant ##ed and excessive emphasis on the theoretical rejection of individual terrorism creating the misleading impression that this is the main only marxist position on terrorism the discussion is not being properly monitored a no disc ##ern ##ible attempt is being made to establish and maintain an acceptable degree of object ##ivity b important and relevant scholarly works such as the international encyclopedia of terrorism are being ignored or illicit ##ly excluded from the discussion c though the only logical way to remedy the b ##lat ##ant im ##balance in the above section is to include quotes by ##on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with imp ##uni ##ty by the ap ##ologists for marxist terrorism who have done their best to sabotage and wreck both the article and the discussion among the tactics deployed by the ap ##ologist wreck ##ers and sa ##bot ##eurs the following may be identified as representative examples a it is claimed that marx and eng ##els did not advocate terrorism despite the fact that scholarly works like the international encyclopedia of terrorism [SEP],[CLS] shelly shock shelly shock is [SEP],[CLS] i do not care refer to on ##g ten ##g che ##ong talk page is la go ##ut ##te de pl ##ui ##e writing a biography or writing the history of trade unions she is making use of the dead to push her agenda again right before elections too how timely [SEP]\n",
       "y: MultiCategoryList\n",
       "toxic,,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: TextList\n",
       "[CLS] gee ##z are you forget ##ful we have already discussed why marx was not an anarchist ie he wanted to use a state to mold his socialist man er ##go he is a stat ##ist the opposite of an anarchist i know a guy who says that when he gets old and his teeth fall out hell quit eating meat would you call him a vegetarian [SEP],[CLS] car ##io ##ca rf ##a thanks for your support on my request for ad ##mins ##hip the final outcome was so i am now an administrator if you have any comments or concerns on my actions as an administrator please let me know thank you [SEP],[CLS] birthday no worries it is what i do enjoy ur day ##talk ##e [SEP],[CLS] pseudo ##sc ##ience category i am assuming that this article is in the pseudo ##sc ##ience category because of it is association with creation ##ism however there are modern scientific ##ally ##ac ##ce ##pt ##ed variants of cat ##ast ##rop ##hism that have nothing to do with creation ##ism and they are even mentioned in the article i think the connection to pseudo ##sc ##ience needs to be clarified or the article made more general and less creation ##isms ##pe ##ci ##fi ##c and the category tag removed entirely [SEP],[CLS] and if such phrase exists it would be provided by search engine even if mentioned page is not available as a whole [SEP]\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7f501d55cd90>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (3): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Dropout(p=0.1)\n",
       "  (1): Linear(in_features=768, out_features=6, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.split(\n",
    "    [x[0], x[1], x[2], x[3], x[5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.083318</td>\n",
       "      <td>0.079288</td>\n",
       "      <td>0.970214</td>\n",
       "      <td>32:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.046989</td>\n",
       "      <td>0.044490</td>\n",
       "      <td>0.978800</td>\n",
       "      <td>32:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(\n",
    "    2,max_lr=slice(1e-5, 5e-4), moms=(0.8, 0.7), pct_start=0.2,\n",
    "    wd=(1e-7, 1e-5, 1e-4, 1e-3, 1e-2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: TextList\n",
       "[CLS] grandma terri should burn in trash grandma terri is trash i hate grandma terri fk her to hell [SEP],[CLS] may utc it would be easiest if you were to admit to being a member of the involved portuguese lodge and then there would be no requirement to acknowledge whether you had a previous account carlos bot ##el ##ho did not have a good record or not and i would then remove the sock ##pu ##ppet template as irrelevant w ##pc ##oi permit is people to edit those articles such as ms ##ja ##pan does but just means you have to be more careful in ensuring that references back your edit is and that np ##ov is upheld [SEP],[CLS] the object ##ivity of this discussion is doubtful none ##xi ##sten ##t as indicated earlier the section on marxist leaders views is misleading a it lays un ##war ##rant ##ed and excessive emphasis on tr ##ots ##ky creating the misleading impression that other prominent marxist ##s marx eng ##els lenin did not advocate and ##or practiced terrorism b it lays un ##war ##rant ##ed and excessive emphasis on the theoretical rejection of individual terrorism creating the misleading impression that this is the main only marxist position on terrorism the discussion is not being properly monitored a no disc ##ern ##ible attempt is being made to establish and maintain an acceptable degree of object ##ivity b important and relevant scholarly works such as the international encyclopedia of terrorism are being ignored or illicit ##ly excluded from the discussion c though the only logical way to remedy the b ##lat ##ant im ##balance in the above section is to include quotes by ##on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with imp ##uni ##ty by the ap ##ologists for marxist terrorism who have done their best to sabotage and wreck both the article and the discussion among the tactics deployed by the ap ##ologist wreck ##ers and sa ##bot ##eurs the following may be identified as representative examples a it is claimed that marx and eng ##els did not advocate terrorism despite the fact that scholarly works like the international encyclopedia of terrorism [SEP],[CLS] shelly shock shelly shock is [SEP],[CLS] i do not care refer to on ##g ten ##g che ##ong talk page is la go ##ut ##te de pl ##ui ##e writing a biography or writing the history of trade unions she is making use of the dead to push her agenda again right before elections too how timely [SEP]\n",
       "y: MultiCategoryList\n",
       "toxic,,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: TextList\n",
       "[CLS] gee ##z are you forget ##ful we have already discussed why marx was not an anarchist ie he wanted to use a state to mold his socialist man er ##go he is a stat ##ist the opposite of an anarchist i know a guy who says that when he gets old and his teeth fall out hell quit eating meat would you call him a vegetarian [SEP],[CLS] car ##io ##ca rf ##a thanks for your support on my request for ad ##mins ##hip the final outcome was so i am now an administrator if you have any comments or concerns on my actions as an administrator please let me know thank you [SEP],[CLS] birthday no worries it is what i do enjoy ur day ##talk ##e [SEP],[CLS] pseudo ##sc ##ience category i am assuming that this article is in the pseudo ##sc ##ience category because of it is association with creation ##ism however there are modern scientific ##ally ##ac ##ce ##pt ##ed variants of cat ##ast ##rop ##hism that have nothing to do with creation ##ism and they are even mentioned in the article i think the connection to pseudo ##sc ##ience needs to be clarified or the article made more general and less creation ##isms ##pe ##ci ##fi ##c and the category tag removed entirely [SEP],[CLS] and if such phrase exists it would be provided by search engine even if mentioned page is not available as a whole [SEP]\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7f501d55cd90>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (3): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Dropout(p=0.1)\n",
       "  (1): Linear(in_features=768, out_features=6, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.save('head')\n",
    "learner.load('head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.044312</td>\n",
       "      <td>0.045877</td>\n",
       "      <td>0.977916</td>\n",
       "      <td>21:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.041469</td>\n",
       "      <td>0.044286</td>\n",
       "      <td>0.979055</td>\n",
       "      <td>21:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.freeze_to(-2)\n",
    "learner.fit_one_cycle(\n",
    "    2, max_lr=slice(1e-5, 5e-4), moms=(0.8,0.7), pct_start=0.2, \n",
    "    wd=(1e-7, 1e-5, 1e-4, 1e-3, 1e-2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: TextList\n",
       "[CLS] grandma terri should burn in trash grandma terri is trash i hate grandma terri fk her to hell [SEP],[CLS] may utc it would be easiest if you were to admit to being a member of the involved portuguese lodge and then there would be no requirement to acknowledge whether you had a previous account carlos bot ##el ##ho did not have a good record or not and i would then remove the sock ##pu ##ppet template as irrelevant w ##pc ##oi permit is people to edit those articles such as ms ##ja ##pan does but just means you have to be more careful in ensuring that references back your edit is and that np ##ov is upheld [SEP],[CLS] the object ##ivity of this discussion is doubtful none ##xi ##sten ##t as indicated earlier the section on marxist leaders views is misleading a it lays un ##war ##rant ##ed and excessive emphasis on tr ##ots ##ky creating the misleading impression that other prominent marxist ##s marx eng ##els lenin did not advocate and ##or practiced terrorism b it lays un ##war ##rant ##ed and excessive emphasis on the theoretical rejection of individual terrorism creating the misleading impression that this is the main only marxist position on terrorism the discussion is not being properly monitored a no disc ##ern ##ible attempt is being made to establish and maintain an acceptable degree of object ##ivity b important and relevant scholarly works such as the international encyclopedia of terrorism are being ignored or illicit ##ly excluded from the discussion c though the only logical way to remedy the b ##lat ##ant im ##balance in the above section is to include quotes by ##on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with imp ##uni ##ty by the ap ##ologists for marxist terrorism who have done their best to sabotage and wreck both the article and the discussion among the tactics deployed by the ap ##ologist wreck ##ers and sa ##bot ##eurs the following may be identified as representative examples a it is claimed that marx and eng ##els did not advocate terrorism despite the fact that scholarly works like the international encyclopedia of terrorism [SEP],[CLS] shelly shock shelly shock is [SEP],[CLS] i do not care refer to on ##g ten ##g che ##ong talk page is la go ##ut ##te de pl ##ui ##e writing a biography or writing the history of trade unions she is making use of the dead to push her agenda again right before elections too how timely [SEP]\n",
       "y: MultiCategoryList\n",
       "toxic,,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: TextList\n",
       "[CLS] gee ##z are you forget ##ful we have already discussed why marx was not an anarchist ie he wanted to use a state to mold his socialist man er ##go he is a stat ##ist the opposite of an anarchist i know a guy who says that when he gets old and his teeth fall out hell quit eating meat would you call him a vegetarian [SEP],[CLS] car ##io ##ca rf ##a thanks for your support on my request for ad ##mins ##hip the final outcome was so i am now an administrator if you have any comments or concerns on my actions as an administrator please let me know thank you [SEP],[CLS] birthday no worries it is what i do enjoy ur day ##talk ##e [SEP],[CLS] pseudo ##sc ##ience category i am assuming that this article is in the pseudo ##sc ##ience category because of it is association with creation ##ism however there are modern scientific ##ally ##ac ##ce ##pt ##ed variants of cat ##ast ##rop ##hism that have nothing to do with creation ##ism and they are even mentioned in the article i think the connection to pseudo ##sc ##ience needs to be clarified or the article made more general and less creation ##isms ##pe ##ci ##fi ##c and the category tag removed entirely [SEP],[CLS] and if such phrase exists it would be provided by search engine even if mentioned page is not available as a whole [SEP]\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7f501d55cd90>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (3): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Dropout(p=0.1)\n",
       "  (1): Linear(in_features=768, out_features=6, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.save('head-2')\n",
    "learner.load('head-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Failed to compute the gradients, there might not be enough points.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.unfreeze()\n",
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.043458</td>\n",
       "      <td>0.043009</td>\n",
       "      <td>0.980554</td>\n",
       "      <td>32:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.040652</td>\n",
       "      <td>0.043174</td>\n",
       "      <td>0.980173</td>\n",
       "      <td>32:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(2, slice(5e-6, 5e-5), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MultiCategory ,\n",
       " tensor([0., 0., 0., 0., 0., 0.]),\n",
       " tensor([1.3163e-02, 1.2372e-05, 5.7785e-04, 6.0383e-05, 6.8194e-04, 1.8358e-04]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'you are so sweet'\n",
    "learner.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MultiCategory toxic;obscene;insult,\n",
       " tensor([1., 0., 1., 0., 1., 0.]),\n",
       " tensor([0.9936, 0.2043, 0.9705, 0.0306, 0.8528, 0.1250]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'you are pathetic piece of shit'\n",
    "learner.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
