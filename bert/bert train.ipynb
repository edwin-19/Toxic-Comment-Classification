{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pretrainedmodels in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (0.7.4)\n",
      "Requirement already satisfied: torchvision in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pretrainedmodels) (0.3.0)\n",
      "Requirement already satisfied: munch in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pretrainedmodels) (2.5.0)\n",
      "Requirement already satisfied: tqdm in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pretrainedmodels) (4.29.1)\n",
      "Requirement already satisfied: torch in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pretrainedmodels) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from torchvision->pretrainedmodels) (1.16.5)\n",
      "Requirement already satisfied: six in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from torchvision->pretrainedmodels) (1.12.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from torchvision->pretrainedmodels) (6.1.0)\n",
      "Requirement already satisfied: pytorch-pretrained-bert in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (0.6.2)\n",
      "Requirement already satisfied: torch>=0.4.1 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.3.0)\n",
      "Requirement already satisfied: boto3 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.10.8)\n",
      "Requirement already satisfied: regex in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2019.11.1)\n",
      "Requirement already satisfied: requests in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.22.0)\n",
      "Requirement already satisfied: numpy in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.16.5)\n",
      "Requirement already satisfied: tqdm in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.29.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.8 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.13.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.25.6)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2019.9.11)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.8->boto3->pytorch-pretrained-bert) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.8->boto3->pytorch-pretrained-bert) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/superceed1/anaconda3/envs/openlbl/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.8->boto3->pytorch-pretrained-bert) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pretrainedmodels\n",
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fast ai \n",
    "import fastai\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.text import *\n",
    "\n",
    "from torchvision.models import *\n",
    "import pretrainedmodels\n",
    "\n",
    "import sys\n",
    "\n",
    "from fastai.callbacks.tracker import EarlyStoppingCallback\n",
    "from fastai.callbacks.tracker import SaveModelCallback\n",
    "from fastai.utils import *\n",
    "from torch.utils import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 21:41:04.560673 140407656929088 file_utils.py:39] PyTorch version 1.1.0 available.\n",
      "I1113 21:41:04.582983 140407656929088 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I1113 21:41:06.002159 140407656929088 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/superceed1/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "bert_tok = BertTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastAiBertTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n",
    "        self._pretrained_tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "    \n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '../data/toxic_comment/cleaned/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_root + 'train.csv')\n",
    "test_df = pd.read_csv(data_root + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 8:2 ratio\n",
    "train_df.fillna('no comment', inplace=True)\n",
    "test_df.fillna('no comment', inplace=True)\n",
    "train, val = train_test_split(\n",
    "    train_df, shuffle=True, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127656, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap BERT VOCAB and BERT tokenizer with fast ai modules\n",
    "fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_tokenizer = Tokenizer(\n",
    "    tok_func=FastAiBertTokenizer(\n",
    "        bert_tok, max_seq_len=256\n",
    "    ), pre_rules=[], post_rules=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create the databunch\n",
    "# Note bert tokenizer and bert vocab used here\n",
    "# Fill in some missing values here\n",
    "label_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "data_bunch_train = TextClasDataBunch.from_df(\n",
    "    \".\", train, val,\n",
    "    tokenizer=fastai_tokenizer,\n",
    "    vocab=fastai_bert_vocab,\n",
    "    include_bos=False,\n",
    "    include_eos=False,\n",
    "    text_cols=\"comment_text\",\n",
    "    label_cols=label_columns,\n",
    "    bs=13,\n",
    "    collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[CLS] if it is not genetic than what is it do wolves stop reg ##urg ##itating food once there are in captivity do ding ##os stop it why do other dog owners report reg ##urg ##itating and by the way the book from elizabeth marshall thomas was translated from english into german and i provided the original title which you obviously failed to notice you are the one who makes</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] from wikipedia ##re ##quest ##ed moves december rock band video game rock band there is no page at rock band or rock band as they are just red ##ire ##cts hence no reason for the video game at the end talk contributions actually there was an article at rock band until somebody decided to merge it a couple of weeks ago and that was not even a clear consensus</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] i have been working for some time on category ##rom ##an bridges but you certainly provide new info here this is what a web ##page says about the al ##con ##eta ##r bridge destroyed in the th century arches left moved in i hope they did not change the arch structure then here the most authoritative photo http ##user ##sser ##vic ##ios ##ret ##eca ##les ##jo ##mic ##oe ##al</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] block you have persistent ##ly criticized me personally in multiple discussions over the last couple weeks despite two warnings i am blocking you for hours please learn to con ##fine your commentary to the issues and not the people involved i have un ##block ##ed you since i have not seen any previous warning the di ##fs i have seen did not seem that bad and in any event</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] hmm ##mm i believe that most of the american churches have dropped the mission part of the title because it was simply was not true anymore the name is probably still in use in africa legally most of our american churches go by apostolic faith church with some churches adding the word trinity to distinguish our organization from others in regard to joe bishops sermon on biblical modest ##y</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_bunch_train.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 21:41:48.524676 140407656929088 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained bert model from pytorch pretrained bert\n",
    "from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification, BertForNextSentencePrediction, BertForMaskedLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 21:41:49.783385 140407656929088 modeling.py:580] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/superceed1/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "I1113 21:41:49.788722 140407656929088 modeling.py:588] extracting archive file /home/superceed1/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmprj670r_9\n",
      "I1113 21:41:52.426493 140407656929088 modeling.py:598] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1113 21:41:54.258394 140407656929088 modeling.py:648] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I1113 21:41:54.258864 140407656929088 modeling.py:651] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "bert_model_class = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', num_labels=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_02 = partial(accuracy_thresh, thresh=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert_model_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learner function\n",
    "from fastai.callback import *\n",
    "\n",
    "learner = Learner(\n",
    "    data_bunch_train, model,\n",
    "    loss_func=loss_func, model_dir='model/', metrics=acc_02\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code will help us in splitting the model into desirable parts which will be helpful for us in Discriminative Learning i.e. setting up different learning rates and weight decays for different parts of the model.\n",
    "def bert_class_split(self) -> List[nn.Module]:\n",
    "    bert = model.bert\n",
    "    embedder = bert.embeddings\n",
    "    pooler = bert.pooler\n",
    "    encoder = bert.encoder\n",
    "    classifier = [model.dropout, model.classifier]\n",
    "    n = len(encoder.layer) // 3\n",
    "    print(n)\n",
    "    groups = [[embedder], list(encoder.layer[:n]), list(encoder.layer[n+1:2*n]), list(encoder.layer[(2*n)+1:]), [pooler], classifier]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "x = bert_class_split(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: TextList\n",
       "[CLS] grandma terri should burn in trash grandma terri is trash i hate grandma terri fk her to hell [SEP],[CLS] may utc it would be easiest if you were to admit to being a member of the involved portuguese lodge and then there would be no requirement to acknowledge whether you had a previous account carlos bot ##el ##ho did not have a good record or not and i would then remove the sock ##pu ##ppet template as irrelevant w ##pc ##oi permit is people to edit those articles such as ms ##ja ##pan does but just means you have to be more careful in ensuring that references back your edit is and that np ##ov is upheld [SEP],[CLS] the object ##ivity of this discussion is doubtful none ##xi ##sten ##t as indicated earlier the section on marxist leaders views is misleading a it lays un ##war ##rant ##ed and excessive emphasis on tr ##ots ##ky creating the misleading impression that other prominent marxist ##s marx eng ##els lenin did not advocate and ##or practiced terrorism b it lays un ##war ##rant ##ed and excessive emphasis on the theoretical rejection of individual terrorism creating the misleading impression that this is the main only marxist position on terrorism the discussion is not being properly monitored a no disc ##ern ##ible attempt is being made to establish and maintain an acceptable degree of object ##ivity b important and relevant scholarly works such as the international encyclopedia of terrorism are being ignored or illicit ##ly excluded from the discussion c though the only logical way to remedy the b ##lat ##ant im ##balance in the above section is to include quotes by ##on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with imp ##uni ##ty by the ap ##ologists for marxist terrorism who have done their best to sabotage and wreck both the article and the discussion among the tactics deployed by the ap ##ologist wreck ##ers and sa ##bot ##eurs the following may be identified as representative examples a it is claimed that marx and eng ##els did not advocate terrorism despite the fact that scholarly works like the international encyclopedia of terrorism [SEP],[CLS] shelly shock shelly shock is [SEP],[CLS] i do not care refer to on ##g ten ##g che ##ong talk page is la go ##ut ##te de pl ##ui ##e writing a biography or writing the history of trade unions she is making use of the dead to push her agenda again right before elections too how timely [SEP]\n",
       "y: MultiCategoryList\n",
       "toxic,,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: TextList\n",
       "[CLS] gee ##z are you forget ##ful we have already discussed why marx was not an anarchist ie he wanted to use a state to mold his socialist man er ##go he is a stat ##ist the opposite of an anarchist i know a guy who says that when he gets old and his teeth fall out hell quit eating meat would you call him a vegetarian [SEP],[CLS] car ##io ##ca rf ##a thanks for your support on my request for ad ##mins ##hip the final outcome was so i am now an administrator if you have any comments or concerns on my actions as an administrator please let me know thank you [SEP],[CLS] birthday no worries it is what i do enjoy ur day ##talk ##e [SEP],[CLS] pseudo ##sc ##ience category i am assuming that this article is in the pseudo ##sc ##ience category because of it is association with creation ##ism however there are modern scientific ##ally ##ac ##ce ##pt ##ed variants of cat ##ast ##rop ##hism that have nothing to do with creation ##ism and they are even mentioned in the article i think the connection to pseudo ##sc ##ience needs to be clarified or the article made more general and less creation ##isms ##pe ##ci ##fi ##c and the category tag removed entirely [SEP],[CLS] and if such phrase exists it would be provided by search engine even if mentioned page is not available as a whole [SEP]\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7fb2a2691d90>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (3): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Dropout(p=0.1)\n",
       "  (1): Linear(in_features=768, out_features=6, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.split(\n",
    "    [x[0], x[1], x[2], x[3], x[5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3zV5fn/8dd1sgMkAQkzLNnIJoCKAooDrXuCo0XrqHWv1v5qrV9btUPrXrhtVVy1iqWCG0FAggwZslHCDDMJIWTdvz/OSTiEBELIJ2fk/Xw8zsNzPuc+51y3h+TKvc05h4iICIAv1AGIiEj4UFIQEZEKSgoiIlJBSUFERCooKYiISIXYUAdwqJo3b+46duwY6jBERCLKnDlztjjn0g9WzrOkYGYvAWcAm51zvat4vgfwMjAQ+L1z7qGavG/Hjh3Jysqq01hFRKKdmf1Yk3Jedh+9Aow+wPPbgJuAGiUDERHxnmdJwTk3Ff8v/uqe3+ycmw0UexWDiIgcmogYaDaza8wsy8yycnJyQh2OiEjUioik4Jwb75zLdM5lpqcfdJxERERqKSKSgoiI1A8lBRERqeDllNQ3gZFAczPLBv4IxAE45541s1ZAFpAClJnZLUAv51yuVzGJiMiBeZYUnHNjD/L8RiDDq88XEYlkn/+wia4tmtCuWXK9fq66j0REwsx3P23nyleyuOHNudT3mTdKCiIiYaSktIy7319IrM+Yv3YHXy2r32n4SgoiImHktRk/snhDLg9f1I+2aUk89tnyem0tKCmIiISJTbmF/OOTZYzols5Z/dpw3cjOzP1pB9NXbK23GJQURETCxH0fLaa4tIz7zj4KM+PCzAxapSTy2GfL6q21oKQgIhIGpi7L4b8LNnD9CV3ocEQjABJiY7huZGdmr9nOzFXVbiVXp5QURETCwP3/XcKRzRtx7Ygj97l+8eB2tGiSwOOfLa+XOJQURERCbO22ApZuyuPyYzqQEBuzz3OJcTFcO6IzM1Zt5dvV3rcWlBREREJs6nL/tNPju1a94eclQ9rTvHECny7Z5HksEXccp4hItJm6LIe2aUl0Tm9U5fNJ8TFMuuk4WqQkeh6LWgoiIiFUXFrGNyu2Mrxbc8ys2nL1kRBASUFEJKTmr91B3p4ShlfTdVTflBREROrB9BVbmLFy/0VoU5fl4DM4tnPzEES1PyUFEZF68MCkJVz/xnfs2lOyz/Wpy7fQv10aqclxIYpsX0oKIiL1YNuuIrbtKuLVGWsqru0oKGJB9g6GdwuPriNQUhAR8Zxzjm27igAYP3UVeYXFAExbsYUyV/1U1FBQUhAR8dju4lL2lJRxWu9W7Cgo5tVv1gDw9bItpCTG0i8jNbQBBlFSEBHx2PYCf8tgZPd0RvVowfipq8gtLGbq8hyO69qc2Jjw+VUcPpGIiESp7YGuo6bJ8dx6cjdyC0v4/fsL2bCzMKy6jkBJQUTEc+XjCc0axdO7bSon92rJxPnrAcJqkBmUFEREPLe9INBSaBQPwC0ndQWgc3oj2qYlhSyuqmjvIxERj1W0FJL9SeGoNqncelI32jULr4QAHrYUzOwlM9tsZgured7M7HEzW2FmC8xsoFexiIiE0vZdRfgMUpL2LlC7+aSunDcwI4RRVc3L7qNXgNEHeP40oGvgdg3wjIexiIiEzLaCIlKT4ojxVb/hXbjwLCk456YCBzoR4mzgNec3E0gzs9ZexSMiEirbdxVXjCeEu1AONLcF1gY9zg5c24+ZXWNmWWaWlZOTUy/BiYjUlW27iirGE8JdRMw+cs6Nd85lOucy09PDa/qWiMjBbC8oUkuhBtYB7YIeZwSuiYhEFbUUauZD4OeBWUhHAzudcxtCGI+ISJ1zzkVUS8GzdQpm9iYwEmhuZtnAH4E4AOfcs8Ak4HRgBVAAXOFVLCIioZK/p4TiUkezRuFxXsLBeJYUnHNjD/K8A6736vNFRMLB9l3+zfCaqvtIRES2Fezd9ygSKCmIiHio8r5H4U5JQUTEQ9sr7XsU7pQUREQ8VL4ZnloKIiLC9oIiYnxGSmJkbEqtpCAi4qFtu4ppmhyPWfhvhgdKCiIintq+qyhi1iiAkoKIiKe2FRRFzBoFUFIQEfGUv6WgpCAiIkTWDqmgpCAi4pmyMsf2guKIWaMASgoiIp7JKyyhtMyRlqyBZhGRBi/S9j0CJQUREc9E2mpmUFIQEfFMpO17BEoKIiKeUfeRiIhU2BFh22aDkoKIiGe27SomPsZHo/iYUIdSY0oKIiIe2b6riKaN4iJmMzxQUhAR8Uyk7XsESgoiIp6JtH2PQElBRMQz2yJs3yPwOCmY2WgzW2pmK8zsriqe72Bmn5nZAjP70swyvIxHRKQ+bd9VFFFrFMDDpGBmMcBTwGlAL2CsmfWqVOwh4DXnXF/gPuBBr+IREalPpWWOHbuL1VIIMgRY4Zxb5ZwrAiYAZ1cq0wv4PHD/iyqeFxGJSDt3F+McNIugzfDA26TQFlgb9Dg7cC3YfOC8wP1zgSZmdkTlNzKza8wsy8yycnJyPAlWRKQuReK+RxD6geY7gBFmNhcYAawDSisXcs6Nd85lOucy09PT6ztGEZFDtj0Ct7gAiPXwvdcB7YIeZwSuVXDOrSfQUjCzxsD5zrkdHsYkIlIvKloKGmiuMBvoamadzCweGAN8GFzAzJqbWXkMvwNe8jAeEZF6U7FDaoS1FDxLCs65EuAGYDKwBHjbObfIzO4zs7MCxUYCS81sGdASuN+reERE6lP5DqmR1lLwsvsI59wkYFKla/cE3X8XeNfLGEREQmH7riIS43wkRdBmeBD6gWYRkai0vaA44haugZKCiIgn/DukKimIiAj+xWupSZG1cA2UFEREPJG/p4QmiZ4O23pCSUFExAN5hSU0TlBLQUREgLzCYrUUREQEnHPk7ymhcYKSgohIg7e7uJQyh1oKIiLiH08AaKykICIiFUlB3UciIpK/x58U1H0kIiLkF5YnBU1JFRFp8PIKiwF1H4mICJC3R2MKIiISUN59lKLuIxERKZ991Cghss5SACUFEZE6l7+nmKS4GGJjIu9XbORFLCIS5vL3lETkwjVQUhARqXN5hZG5bTYoKYiI1Lm8whKaRODMI1BSEBGpc+o+EhGRCvmFkbltNnicFMxstJktNbMVZnZXFc+3N7MvzGyumS0ws9O9jEdEpD74j+KMvDUK4GFSMLMY4CngNKAXMNbMelUqdjfwtnNuADAGeNqreERE6ktuYXHEthS8jHoIsMI5twrAzCYAZwOLg8o4ICVwPxVY71Uw89fu4KkvVtC9VRP/rWUTOjZvRFwEziMWkfBVfupapM4+8jLqtsDaoMfZwNBKZe4FppjZjUAj4KSq3sjMrgGuAWjfvn2tgtleUMTKnHw++2EzpWUOgIRYH/3apTG0UzMGd2zGgPZp1Tb5CotLKSoti8hl6yJSfwqKSnEuMvc9Am+TQk2MBV5xzj1sZscA/zSz3s65suBCzrnxwHiAzMxMV5sPGtm9BSO7t6CwuJRVObtYuimXhetymb1mG09/uZLSshUAJMXF0KxRPM0bx5MQF8OW/D3k5O2pWLbeNDmOjs0b0al5IzKaJpOWFEdqUhxpyXEkxcXggDLnKHPQJjWRri2b1P7/johEnL1nKUTmH5BeJoV1QLugxxmBa8F+CYwGcM7NMLNEoDmw2augEuNi6NUmhV5tUjh3gP9a/p4SvvtxO4vW57I1fw/bdhWxZVcRhUWl9GyVwvCuCaQ3SSAuxliztYDVObuYsXIrG3ZWrs7+erdN4YKBGZzVvy3NGsV7VS0RCRMV22ar+2g/s4GuZtYJfzIYA1xSqcxPwCjgFTPrCSQCOR7GVKXGCbEM75bO8G7ph/S60jJHXmExOwqK2bm7mN3FpfjM8BmYwffZO3lnTjb3TlzM/ZOWMLxrOif1asmoHi1okZLoUW1EJJTKexUidfFajaI2s85AtnNuj5mNBPoCrznndlT3GudciZndAEwGYoCXnHOLzOw+IMs59yFwO/C8md2Kf9B5nHOuVt1DoRDjM9KS40lLrroFMKhDM8YN68Ti9bm89102Hy/cyGc/+BtBfTNSOabzEfRqnULP1il00qC3SFQo7z6K9pbCe0CmmXXB37f/AfAGcMB1Bc65ScCkStfuCbq/GBh2KAFHIn93VS/u/llPlm7K47Mlm/l0ySZemraa4lJ/DoyP8dGzTQqZHZoyqENTMjs0VWtCJALtPYozupNCWeAv/3OBJ5xzT5jZXC8Di0ZmRo9WKfRolcL1J3ShqKSMlTn5/LAxl8Xrc5m/dif/mvkjL05bDUCv1ilcmJnB2RqPEIkY5d1H0T77qNjMxgK/AM4MXIvMofUwEh/ro2eg+6h80LuopIyF63cye/U2Plqwgf+buJgHJi3hpJ4tuWhwO4Z3TSfGZ6ENXESqVX4UZ5OEyPwVWdOkcAXwK+B+59zqwODxP70Lq+GKj/UxsH1TBrZvyrUjOrNkQy7vzsnmP3PX8b+FG2mblsSFmRlclNmONmlJoQ5XRCop7z6K1DEFO9RxXTNrCrRzzi3wJqQDy8zMdFlZWaH46JAqKinj0yWbePPbn/h6+RZ8BiO6pXPx4PaM6tlCg9QiYeLPHy3mjW9/YvF9o0Mdyj7MbI5zLvNg5Wo6++hL4KxA+TnAZjOb7py77bCilBqLj/Vxep/WnN6nNWu3FfDW7LW8M2ctv/rXHJo3juf8gRlcOrQD7Y9IDnWoIg1a/p7I3SEVar4hXqpzLhc4D/9U1KFUsyWFeK9ds2TuOLU70397Ii+Ny2Rg+6a8OG01Ix/6gutf/475a6udKSwiHsuL4LMUoOZjCrFm1hq4CPi9h/HIIYiN8XFij5ac2KMlm3ILeXn6Gl6f9SP//X4DQzo1445TujOkU7NQhynSoOQXRu622VDzlsJ9+BehrXTOzTazI4Hl3oUlh6plSiJ3ndaDGb8bxd0/68lPWwu46LkZ3PTmXDbuLAx1eCINRl5hccSuZoYaJgXn3DvOub7OuesCj1c55873NjSpjcYJsVx1/JF8ccdIbhrVlY8XbeTEh7/k6S9XUFhcGurwRKJegxhTMLMMM3vfzDYHbu+ZWYbXwUntJcXHcNvJ3fjsthEc16U5f/t4KaMe/op3stZWbB0uInUvvzCyxxRq2n30MvAh0CZwmxi4JmGuXbNkxv88kzeuGkrzxvHc+e4CRj86lcmLNhJB20yJRIy8CD5gB2qeFNKdcy8750oCt1eAQ9tSVELq2C7N+c/1w3j2soGUOse1/5zD2OdnsmxTXqhDE4kaZWWBU9eivfsI2Gpml5lZTOB2GbDVy8Ck7pkZo3u3Zsotw/nzOb35YWMepz/2NX/+aHHFHvAiUnsFxYFT1xpAS+FK/NNRNwIbgAuAcR7FJB6LjfFx2dEd+Pz2kVyYmcGL01cz6uGveG9ONmUabxCptYotLiJ03yOo+eyjH51zZznn0p1zLZxz5wCafRThmjWK58Hz+vL+r4fROjWR29+Zz7nPfMN3P20PdWgiESl/j7/F3RDGFKqiLS6iRP92abz/62E8fGE/NuzYzXlPf8MtE+aydltBqEMTiSi5Eb4ZHhzecZzavzmK+HzG+YMyGN27FU9/uYLnv17Nf7/fwJjB7bnhxC601IE/IgeVH+FHccLhtRTU+RyFGiXEcuepPfjqzpFclNmON7/9ieF/+4I/f7SY7O1qOYgcSKQfxQkHaSmYWR5V//I3QJv5R7HWqUncf24frh3emcc/X85L01fz4vTVjOyWziVDO3BC93RitV23yD72HsUZuQPNB0wKzrkm9RWIhKf2RyTz0IX9uPXkbrz17U9MmL2Wq1/Lok1qIted0IWLM9sRH6vkIAKQG5jaHfXbXIi0TUvitlO6M/2uE3n2skG0SUviD/9ZyKh/fMm/v8vW1hkiBHUfKSlIQxEX42N071a886tjePmKwaQkxnHb2/MZ/ehUPlqwXuscpEHLLywhOT4mos9R9zQpmNloM1tqZivM7K4qnn/EzOYFbsvMTKfDRAgz44TuLZh4w3E8felAAG54Yy6nKjlIA5ZXGNn7HsHhTUk9IDOLAZ4CTgaygdlm9qFzbnF5GefcrUHlbwQGeBWPeMPnM07v05pTj2rFpO838Phny7nhjbl0a7mcO07pzsm9WmIWuX81iRyKSN82G7xtKQwBVgTOXigCJgBnH6D8WOBND+MRD8X4jDP7tWHyLcN5YuwASsoc1/xzDhc8O4PZa7aFOjyReuE/ijNyZx6Bt0mhLbA26HF24Np+zKwD0An4vJrnrzGzLDPLysnJqfNApe74Aslhyi3DefC8PqzdVsCFz87gyldmM2vVVm3XLVEtv7CYlAjvPgqXgeYxwLvOuSqPBnPOjXfOZTrnMtPTtWN3JIiN8TF2SHu+uvME7jy1O3N/2s7F42dy5pPTeH9uNkUlZaEOUaTO5RWq++hA1gHtgh5nBK5VZQzqOopKSfExXH9CF765axQPnNuH3UWl3PrWfEb8/QvemPUTxaVKDhI9NKZwYLOBrmbWyczi8f/i/7ByITPrATQFZngYi4RYUnwMlwxtzye3juDlKwbTOjWR//f+95z0j6/4YN46zVaSqBDpR3GCh0nBOVcC3ABMBpYAbzvnFpnZfWZ2VlDRMcAEp87mBsHn809lfe+6Y3nxF5kkxcVw84R5/OyJacz5UVt2S+QqK3PkF5VE9BYX4OGUVADn3CRgUqVr91R6fK+XMUh4MjNG9WzpX+uwYD1/+d8PnP/MN4wd0p67RvcgNTmyf7Ck4dlVVIJzkb1DKoTPQLM0UD6fcXb/tnxy2wiuOq4Tb2etZdQ/vuT9udmaqSQRJRp2SAUlBQkTjRNiufuMXnx4wzDaNk3m1rfmc+kLs1iVkx/q0ERqZO9RnEoKInXmqDap/Pu6Y/nTOb35ft1ORj/6NY98sozC4ipnK4uEjbw95dtmKymI1KkYn3H50R347PYRnNanFY99tpzTH/+aBdnaGkvCV16hkoKIp1o0SeSxMQN47coh7C4q5bynv+GJz5ZTorUNEob2dh9F9iQJJQUJe8O7pfPxzcM5vU9rHv5kGRc9N4Mft+4KdVjSwL02Yw03vjm3YgFm/p7AATtqKYh4LzU5jsfHDuCxMf1ZsTmf0x/7mg/mVbdAXsRbzjme+2oVE+ev56//+wFQ95FISJzdvy0f3zKcXm1SuHnCPO58Zz4FRSWhDksamEXrc1m3YzdHpjfihWmr+WjB+oqk0CheSUGkXrVJS+LNq4/mxhO78O532Zz5xDSWbMgNdVjSgExetBGfwZtXH82gDk35zbsLmLd2B40i/NQ1UFKQCBUb4+P2U7rz+i+HkltYwrlPT1d3ktSbyYs2MqRTM1qmJPL0pQNJjo/lq2U5ET+eAEoKEuGO7dKcSTcdT9+2adw8YR73TVysnVfr0WdLNrF4fcNqpa3MyWfZpnxOPaoVAC1TEnnqkgHE+CziF66Bx3sfidSH9CYJvH71UO7/7xJemr6aRet38tSlA2neOCHUoUW937y7ADPjfzcfT3qThvH/e/KijQAVSQFg6JFH8OjF/dldFPmLLNVSkKgQF+Pj3rOO4h8X9WPe2h2MfnQqnyzeFOqwolphcSlbdxWxJX8Pt709r8Fsfz550Sb6ZaTSJi1pn+tn9mvDRYPbVfOqyKGkIFHlvIEZ/Of6YaQ3SeTq17K445355BYWhzqsqLQptxCAIZ2a8fXyLYz/elWIIzo0zrlqZ66VlTnemv0T9/933+7IDTt3M3/tDk4JaiVEGyUFiTo9W6fwwfXDuPHELrw/dx2jH5nKNyu2hDqsqLNhpz8p3HRiV07v04qHJi/lu58i50yMt2avpfcfJ3PrW/NYsTmv4vqKzXlcPH4Gv33ve57/ejW3vz2f0kAraMoif+tzdG8lBZGIEh/rn5303nXHkhQfw6UvzuIfU5ZW/HDL4dsYSAqt0xJ58Ly+tEpN5KY357Jzd2S0zD5dsplG8bF8vHAjJz8yletf/46/fvwDpz32Ncs25fO38/ty56nd+XD+eu75YCHOOT5euJEuLRrTOb1xqMP3jAaaJar1b5fGxBuP454PFvH45yuYtXobj48dQMuUxFCHFvHKWwqtUhJplBDL42MHcNGzM/jtuwt45rKBmIXvfP3SMse3q7fys76tufPU7rw0fTWvfvMj+XtKOKd/G+4+o1fFRIXcwmKe+2oVPjO+XbON60Z0DnH03lJSkKiXHB/LQxf245gjj+Du/yzk9Me+5omxAzi2S/NQhxbRNu7cTUpiLI0C0zAHtm/Kb0Z354FJP/Dy9DVceVynEEdYvR825pJbWMLQI5txROME7jy1B9cc35ktu/bs1wq4a3QPcncX88+ZPwLR3XUE6j6SBuT8QRlMvPE4jmgczy9e/paJ89eHOqSItmFnIa1T952Bc/XxR3Jyr5Y8MGlJWI8vzFy1DYChnY6ouJaaHFdlt5CZ8edz+nDBoAwGtk/jqDYp9RZnKCgpSIPSpUVj3vnVsQxo15SbJszllemrQx1SxNqws5BWqft2w5kZD13Qj1apidzw+nds31UUougObNaqrbRvlrzftNLqxPiMhy7sx3vXHRvW3WJ1QUlBGpzUpDhe++UQTu7ZknsnLuahyUt1HnQt+FsK+4/NpCbH8fSlA9mSXxSW6xfKyhzfrtnG0E7NDvm10Z4QQElBGqjEuBievnQgY4e048kvVnD3fxaG3S+vcFZUUsaW/D37tRTK9c1I4w9n9OSLpTlc/VoWWWu2hU3iXbopjx0FxRx95BEHL9wAeZoUzGy0mS01sxVmdlc1ZS4ys8VmtsjM3vAyHpFgsTE+Hji3D78a0ZnXZ/3E/3v/eyWGGipfuFZVS6HcZUd34M5Tu5P143YueHYG5z79DR8tWB/yacGzVm0FYOiRh95SaAg8m31kZjHAU8DJQDYw28w+dM4tDirTFfgdMMw5t93MWngVj0hVzIzfju5OXIzxxOcrKClz/PX8vhG//bHXNgaSQqvU6vvkzYzrT+jCFcM68t6cbF6ctpob3pjLRZk5/PX8viHripm5ahtt05LIaJocks8Pd15OSR0CrHDOrQIwswnA2cDioDJXA08557YDOOc2exiPSJXMjNtP6U6Mz3j00+WUlTn+fmE/JYYDKF+j0OYALYVyyfGxXH5MRy4Z2oGHpizlmS9X0qdtKpcf09HjKPfnnH884YTu+vuzOl4mhbbA2qDH2cDQSmW6AZjZdCAGuNc593HlNzKza4BrANq3b+9JsCK3nNSNGDMe/mQZZsbfL+iLT4mhSht37gaodkyhKjE+485TurN0Yx7/N3Ex3VulMKQWg72HY/nmfLbtKlLX0QGEeqA5FugKjATGAs+bWVrlQs658c65TOdcZnp6ej2HKA3JjaO6cstJXXnvu2zun7QkbAZHw82GnYU0ToilSWLcIb3O5zMeubg/7Zol8+vX57AhkFzqy8zAeMLRnTTIXB0vk8I6IHgf2YzAtWDZwIfOuWLn3GpgGf4kIRIyN4/qyrhjO/LitNU89cWKUIcTljZWsUahplKT4hh/+SB2F5Xyq399R2Fx/Z1BMGvVNtqkJtKuWc3WJzREXiaF2UBXM+tkZvHAGODDSmX+g7+VgJk1x9+dFFn770rUMTPuOaMX5w1oy0NTllVsbyB7VbdGoaa6tmzCwxf1929D/chU3p+b7fmsJOccs1ZvZeiRRzSI9Qa15VlScM6VADcAk4ElwNvOuUVmdp+ZnRUoNhnYamaLgS+AO51zW72KSaSmfD7jrxf05aSeLbjng4W8k7X24C9qQDbuLKTVYW4qOLp3K169cgiNE2K59a35nPbYVCYv2uhZl93KnHy25BfVatFaQ+LphnjOuUnApErX7gm674DbAjeRsBIX4+PJSwZy1atZ3PnuAvIKS8J6k7f6UlJaxua8w2splBvRLZ3juzRn0sIN/GPKMq795xyGdGzGPWf2onfb1DqIdq8pgZP4tGjtwEI90CwS1hLjYnhxXCajj2rFfR8t5pFPljX4weec/D2UuQOvUTgUPp9xRt82TLl1OA+c24cVOfmc+eQ0fvfvBWzJ31Mnn5FXWMzzU1cxvFs6HZs3qpP3jFZKCiIHkRAbw5OXDODCQRk89tly/m/i4ga98rl8jUJdtBSCxcb4uGRoe764YyS/HNaJd7KyOeHvX9bJbrYvT1/D9oJi7jilWx1EGt2UFERqIDbGx98u6MtVx3XilW/WcN9Hiw/+oihVfuJabWcfHUxqUhx3n9GLybcOp1urJtz45lz+9vEPtR6I3lFQxPNTV3FKr5b0zdhvxrtUoqQgUkNmxu9/1pMrh/kTw0vTGua22161FCrrnN6YN68+mrFD2vP0lyu56tXZ5BYe+lGf46euIr+ohNvUSqgRJQWRQ1CeGE49qiV/+u9iJi/aGOqQ6t3GnbtJjPORmnRoC9dqIz7Wx4Pn9eHP5/Tm6+VbOPvJ6bw4bTVLNuTWqAtvS/4eXp6+hjP6tqFHq+g+HKeu6DhOkUMU4zMevXgAY56fyc0T5jLhmmPo367hdEuUn7hWn3P9Lzu6A11bNOZ373/PnwJdd80axZPZoSlpyXEkxMYQH+ujcUIsA9qnMbhjMxolxPLMlyvZU1LKrSdpTWxNKSmI1EJSfAwv/DyT856ZzlWvzmbCNcfQpcX+RzlGmqUb8yguLTvgdNC6WKNQG0OPPILPbx/Juh27mbFyK9+s3MK8n3ZQUFTKnpJSikrKKCguxTmI9Rl9M1JZuD6X8wdmcGQVx2xK1ZQURGopvUkCL48bwsXPzeCcp6bz8EX9OPWoyD7U/Q8fLGThup28fe0x1SaGDTsLQ7oArG1aEhcMyuCCQRn7PVdQVELWmu3MWLWVGSu30iw5nptGqZVwKJQURA5DlxaN+fDG4/j1v+Zw7T/ncP0Jnbnt5O4Ru+32uu27KSgq5apXs/jP9cP2m2FUVubYlFv7fY+8lhwfy/Bu6Qzvpo0za0sDzSKHqW1aEm9dewxjBrfjqS9WMu7lb9m5+9BnyYRaaeAX/kk9W5BXWMxVr82moKhknzJbdu2hpMx5PvNIQkdJQaQOJMbF8Jfz+/KX8wz+EIsAAA5VSURBVPowc9VWLn9xVsQlhq35/l/4I7q34IlLBrB4fS63TJi3zyyfvWsUtMtotFJSEKlDY4a059nLBrFkQy4/f3FWrebVh8r68vUHKYmc2KMlfzijF1MWb+KBSUsqytTXGgUJHSUFkTo2qmdLnrl0EIs35HL5i99GTGIoP02tdZr/F/64Yzsy7tiOvDBtNc99tTJQxtvVzBJ6SgoiHjipV0ueumQgi9fv5OcRkhjW7yhvBfi7hsrPlTijb2se/N8PvDsnmw07C4mP8dEsOT6UoYqHlBREPHLKUa148pKBLFwXGYlhY24hCbE+mibvXans8xn/uKg/x3dtzm/fW8CURRtpmZqgs6ujmJKCiIdOPaoVT186kEXrd3L5C+E9+Lx+x25apybut1I5PtbHM5cN4qg2KazasquiJSHRSUlBxGOnHNWKZy4dxJINeVz2wix2FBSFOqQqbQxsX1GVxgmxvDxuMN1bNqFPHR9+I+FFSUGkHpzUqyXPXT6IpRvzuPSFWWzfFX6J4WDnLh/ROIH/3Xw8d/+sZz1GJfVNSUGknpzQowXjfz6I5ZvzueSFWWwLo8RQvnCtfOZRdXw+06H3UU5JQaQejezeghd+nsmqnHwueX5mnR03ebjKF65pUZooKYjUs+Hd0nlp3GDWbN3F2PEz2ZxXGOqQ9lm4Jg2bp0nBzEab2VIzW2Fmd1Xx/DgzyzGzeYHbVV7GIxIuhnVpzitXDGHdjt2MeW4mH8xbR2FxacjiqbxwTRouz5KCmcUATwGnAb2AsWbWq4qibznn+gduL3gVj0i4OfrII3j1yiHsKSnj5gnzGHz/p/z+/e/5PntnvcdSeeGaNFxethSGACucc6ucc0XABOBsDz9PJOIM7tiMr39zAq9fNZRRPVrw7pxsznxyGm/PXluvcVS1cE0aJi+TQlsg+F92duBaZeeb2QIze9fM2nkYj0hY8vmMYV2a8+iYAcy++ySO79qcu/69gI8WrK+3GMqno2pmkYR6oHki0NE51xf4BHi1qkJmdo2ZZZlZVk5OTr0GKFKfUhLjGH95JpkdmnHLhHl8tmRTvXzuhh271XUkgLdJYR0Q/Jd/RuBaBefcVudc+Zy8F4BBVb2Rc268cy7TOZeZnq4TlSS6JcXH8OK4THq1SeG6179j+ootnn/mwRauScPhZVKYDXQ1s05mFg+MAT4MLmBmrYMengUsQURokhjHq1cModMRjbjq1SxPWwylYX7EptQvz5KCc64EuAGYjP+X/dvOuUVmdp+ZnRUodpOZLTKz+cBNwDiv4hGJNE0bxfOvq4bSpUVjrn4ti1emr/bkc8oXrrVOU/eRQKyXb+6cmwRMqnTtnqD7vwN+52UMIpEsvUkCb117NDdPmMe9ExezZmsBfzijFzE+o7C4lBWb84mL8dG9VZNaf4YWrkkwT5OCiBy+5PhYnr1sEA9OWsIL01Yzc9VWdheX8tO2Alzg+OQ7TunG9Sd0qdXsIS1ck2BKCiIRIMZn3H1GLzqlN+Kt2WvpnN6Ycwe0pVvLJkxZtJGHpixjZc4uHjyvD4lxMYf03nvPXVb3kSgpiESUS4d24NKhHfa5dlrvVnRp0ZiHpizjp20FPHf5IJo3Tqjxe27YqYVrsleo1ymIyGEyM244sWvFCW9nPTGNrDXbavx6LVyTYEoKIlHi9D6teefaY4mL9XHRczN49NNllJSWHfR1WrgmwZQURKJIn4xUPrrxOM7u35ZHP13O2Odnsm7H7gO+RgvXJJiSgkiUaZIYxyMX9+eRi/uxeH0uP3v8a2at2lplWS1ck8qUFESi1LkDMvjopuNp1iiey16cxTtZ+++8qoVrUpmSgkgU69S8Ee9fN4whnZpx57sL+OvHP1BW5iqe18I1qUxJQSTKpSbH8coVQ7hkaHue+XIl1/5rDjt3FwNauCb7U1IQaQDiYnzcf05v/nhmL774YTM/e/xr5q3doYVrsh8tXhNpIMyMK4Z1ol+7NG58Yy4XPvsNndMba+Ga7EMtBZEGZmD7pky66XhGdm/BDxvztHBN9qGWgkgDlJocx/jLB/F21lqS4vVrQPbSvwaRBsrMuHhw+1CHIWFG3UciIlJBSUFERCooKYiISAUlBRERqaCkICIiFZQURESkgpKCiIhUUFIQEZEK5pw7eKkwYmY5wI9VPJUK7Kzl4/L75f9tDmypZYiVP+dQylR1vSZxB98PvuZlPbysQ/D9hv5dhLoOwffD5bvQz3bt6tHBOZd+0FLOuai4AeNr+7j8ftB/s+oqjkMpU9X1msRdVR28roeXddB3ET51CMfvQj/bh1ePg92iqfto4mE8nlhNmbqI41DKVHW9JnEH36+LOtTkfbysQ00+vyai4bsIdR1qGsPB1GU99LPtoYjrPqoPZpblnMsMdRyHKxrqEQ11gOioh+oQPrysRzS1FOrS+FAHUEeioR7RUAeIjnqoDuHDs3qopSAiIhXUUhARkQpKCiIiUiHqk4KZvWRmm81sYS1eO8jMvjezFWb2uAWdWWhmN5rZD2a2yMz+VrdR7xdHndfBzO41s3VmNi9wO73uI98vFk++i8Dzt5uZM7PmdRdxlXF48V38ycwWBL6HKWbWpu4j3y8WL+rx98DPxAIze9/M0uo+8n3i8KIOFwZ+psvMzLMB6cOJvZr3+4WZLQ/cfhF0/YA/N1Xyaq5ruNyA4cBAYGEtXvstcDRgwP+A0wLXTwA+BRICj1tEYB3uBe6I9O8i8Fw7YDL+RY3NI60OQEpQmZuAZyPxuwBOAWID9/8K/DUC69AT6A58CWSGW+yBuDpWutYMWBX4b9PA/aYHqueBblHfUnDOTQW2BV8zs85m9rGZzTGzr82sR+XXmVlr/D+sM53//+5rwDmBp68D/uKc2xP4jM0RWId652E9HgF+A3g+a8KLOjjncoOKNiJy6zHFOVcSKDoTyIjAOixxzi31Mu7Dib0apwKfOOe2Oee2A58Ao2v78x/1SaEa44EbnXODgDuAp6so0xbIDnqcHbgG0A043sxmmdlXZjbY02irdrh1ALgh0NR/ycyaehfqAR1WPczsbGCdc26+14EewGF/F2Z2v5mtBS4F7vEw1gOpi39T5a7E/5dpfavLOtS3msRelbbA2qDH5fWpVT1ja/ihUcPMGgPHAu8Eda8lHOLbxOJvqh0NDAbeNrMjA9nYc3VUh2eAP+H/q/RPwMP4f5DrzeHWw8ySgf+Hv9siJOrou8A593vg92b2O+AG4I91FmQN1FU9Au/1e6AEeL1uoqvx59ZZHerbgWI3syuAmwPXugCTzKwIWO2cO7euY2lwSQF/62iHc65/8EUziwHmBB5+iP+XZnDzNwNYF7ifDfw7kAS+NbMy/BtU5XgZeJDDroNzblPQ654HPvIy4Gocbj06A52A+YEfpAzgOzMb4pzb6HHs5eri31Ow14FJ1HNSoI7qYWbjgDOAUfX1R1KQuv4u6lOVsQM4514GXgYwsy+Bcc65NUFF1gEjgx5n4B97WEdt6unVQEo43YCOBA3oAN8AFwbuG9CvmtdVHqQ5PXD9V8B9gfvd8DfdLMLq0DqozK3AhEj8LiqVWYPHA80efRddg8rcCLwbid8FMBpYDKTXR/xe/nvC44Hm2sZO9QPNq/EPMjcN3G9Wk3pWGVd9fXmhugFvAhuAYvx/4f8S/1+XHwPzA/+I76nmtZnAQmAl8CR7V4DHA/8KPPcdcGIE1uGfwPfAAvx/PbX2sg5e1aNSmTV4P/vIi+/ivcD1Bfg3PWsbid8FsAL/H0jzAjdPZ1F5VIdzA++1B9gETA6n2KkiKQSuXxn4/78CuOJQfm4q37TNhYiIVGios49ERKQKSgoiIlJBSUFERCooKYiISAUlBRERqaCkIFHBzPLr+fNeMLNedfRepebfIXWhmU082O6iZpZmZr+ui88WqUxTUiUqmFm+c65xHb5frNu7uZungmM3s1eBZc65+w9QviPwkXOud33EJw2LWgoStcws3czeM7PZgduwwPUhZjbDzOaa2Tdm1j1wfZyZfWhmnwOfmdlIM/vSzN41/zkBr5fvRx+4nhm4nx/Y0G6+mc00s5aB650Dj783sz/XsDUzg72b/TU2s8/M7LvAe5wdKPMXoHOgdfH3QNk7A3VcYGb/V4f/G6WBUVKQaPYY8IhzbjBwPvBC4PoPwPHOuQH4dyR9IOg1A4ELnHMjAo8HALcAvYAjgWFVfE4jYKZzrh8wFbg66PMfc871Yd/dKqsU2KNnFP4V5gCFwLnOuYH4z/B4OJCU7gJWOuf6O+fuNLNTgK7AEKA/MMjMhh/s80Sq0hA3xJOG4ySgV9CukymB3ShTgVfNrCv+XWLjgl7ziXMueJ/7b51z2QBmNg//fjXTKn1OEXs3FJwDnBy4fwx7969/A3iomjiTAu/dFliCfz988O9X80DgF3xZ4PmWVbz+lMBtbuBxY/xJYmo1nydSLSUFiWY+4GjnXGHwRTN7EvjCOXduoH/+y6Cnd1V6jz1B90up+mem2O0dnKuuzIHsds71D2wFPhm4Hngc/9kK6cAg51yxma0BEqt4vQEPOueeO8TPFdmPuo8kmk3Bv+soAGZWvi1xKnu3EB7n4efPxN9tBTDmYIWdcwX4j+O83cxi8ce5OZAQTgA6BIrmAU2CXjoZuDLQCsLM2ppZizqqgzQwSgoSLZLNLDvodhv+X7CZgcHXxfi3PAf4G/Cgmc3F29byLcBtZrYA/+EoOw/2AufcXPy7pY7Ff7ZCppl9D/wc/1gIzrmtwPTAFNa/O+em4O+emhEo+y77Jg2RGtOUVBGPBLqDdjvnnJmNAcY6584+2OtEQkljCiLeGQQ8GZgxtIN6Pu5UpDbUUhARkQoaUxARkQpKCiIiUkFJQUREKigpiIhIBSUFERGp8P8BdHhybbfDMZQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.063386</td>\n",
       "      <td>0.056259</td>\n",
       "      <td>0.979912</td>\n",
       "      <td>32:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.039420</td>\n",
       "      <td>0.045312</td>\n",
       "      <td>0.979305</td>\n",
       "      <td>39:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(\n",
    "    2,max_lr=slice(1e-5, 5e-4), moms=(0.8, 0.7), pct_start=0.2,\n",
    "    wd=(1e-7, 1e-5, 1e-4, 1e-3, 1e-2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: TextList\n",
       "[CLS] grandma terri should burn in trash grandma terri is trash i hate grandma terri fk her to hell [SEP],[CLS] may utc it would be easiest if you were to admit to being a member of the involved portuguese lodge and then there would be no requirement to acknowledge whether you had a previous account carlos bot ##el ##ho did not have a good record or not and i would then remove the sock ##pu ##ppet template as irrelevant w ##pc ##oi permit is people to edit those articles such as ms ##ja ##pan does but just means you have to be more careful in ensuring that references back your edit is and that np ##ov is upheld [SEP],[CLS] the object ##ivity of this discussion is doubtful none ##xi ##sten ##t as indicated earlier the section on marxist leaders views is misleading a it lays un ##war ##rant ##ed and excessive emphasis on tr ##ots ##ky creating the misleading impression that other prominent marxist ##s marx eng ##els lenin did not advocate and ##or practiced terrorism b it lays un ##war ##rant ##ed and excessive emphasis on the theoretical rejection of individual terrorism creating the misleading impression that this is the main only marxist position on terrorism the discussion is not being properly monitored a no disc ##ern ##ible attempt is being made to establish and maintain an acceptable degree of object ##ivity b important and relevant scholarly works such as the international encyclopedia of terrorism are being ignored or illicit ##ly excluded from the discussion c though the only logical way to remedy the b ##lat ##ant im ##balance in the above section is to include quotes by ##on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with imp ##uni ##ty by the ap ##ologists for marxist terrorism who have done their best to sabotage and wreck both the article and the discussion among the tactics deployed by the ap ##ologist wreck ##ers and sa ##bot ##eurs the following may be identified as representative examples a it is claimed that marx and eng ##els did not advocate terrorism despite the fact that scholarly works like the international encyclopedia of terrorism [SEP],[CLS] shelly shock shelly shock is [SEP],[CLS] i do not care refer to on ##g ten ##g che ##ong talk page is la go ##ut ##te de pl ##ui ##e writing a biography or writing the history of trade unions she is making use of the dead to push her agenda again right before elections too how timely [SEP]\n",
       "y: MultiCategoryList\n",
       "toxic,,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: TextList\n",
       "[CLS] gee ##z are you forget ##ful we have already discussed why marx was not an anarchist ie he wanted to use a state to mold his socialist man er ##go he is a stat ##ist the opposite of an anarchist i know a guy who says that when he gets old and his teeth fall out hell quit eating meat would you call him a vegetarian [SEP],[CLS] car ##io ##ca rf ##a thanks for your support on my request for ad ##mins ##hip the final outcome was so i am now an administrator if you have any comments or concerns on my actions as an administrator please let me know thank you [SEP],[CLS] birthday no worries it is what i do enjoy ur day ##talk ##e [SEP],[CLS] pseudo ##sc ##ience category i am assuming that this article is in the pseudo ##sc ##ience category because of it is association with creation ##ism however there are modern scientific ##ally ##ac ##ce ##pt ##ed variants of cat ##ast ##rop ##hism that have nothing to do with creation ##ism and they are even mentioned in the article i think the connection to pseudo ##sc ##ience needs to be clarified or the article made more general and less creation ##isms ##pe ##ci ##fi ##c and the category tag removed entirely [SEP],[CLS] and if such phrase exists it would be provided by search engine even if mentioned page is not available as a whole [SEP]\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7fb2a2691d90>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (3): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Dropout(p=0.1)\n",
       "  (1): Linear(in_features=768, out_features=6, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.save('head')\n",
    "learner.load('head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.035256</td>\n",
       "      <td>0.048823</td>\n",
       "      <td>0.980250</td>\n",
       "      <td>24:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.045283</td>\n",
       "      <td>0.045386</td>\n",
       "      <td>0.979050</td>\n",
       "      <td>22:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.freeze_to(-2)\n",
    "learner.fit_one_cycle(\n",
    "    2, max_lr=slice(1e-5, 5e-4), moms=(0.8,0.7), pct_start=0.2, \n",
    "    wd=(1e-7, 1e-5, 1e-4, 1e-3, 1e-2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: TextList\n",
       "[CLS] grandma terri should burn in trash grandma terri is trash i hate grandma terri fk her to hell [SEP],[CLS] may utc it would be easiest if you were to admit to being a member of the involved portuguese lodge and then there would be no requirement to acknowledge whether you had a previous account carlos bot ##el ##ho did not have a good record or not and i would then remove the sock ##pu ##ppet template as irrelevant w ##pc ##oi permit is people to edit those articles such as ms ##ja ##pan does but just means you have to be more careful in ensuring that references back your edit is and that np ##ov is upheld [SEP],[CLS] the object ##ivity of this discussion is doubtful none ##xi ##sten ##t as indicated earlier the section on marxist leaders views is misleading a it lays un ##war ##rant ##ed and excessive emphasis on tr ##ots ##ky creating the misleading impression that other prominent marxist ##s marx eng ##els lenin did not advocate and ##or practiced terrorism b it lays un ##war ##rant ##ed and excessive emphasis on the theoretical rejection of individual terrorism creating the misleading impression that this is the main only marxist position on terrorism the discussion is not being properly monitored a no disc ##ern ##ible attempt is being made to establish and maintain an acceptable degree of object ##ivity b important and relevant scholarly works such as the international encyclopedia of terrorism are being ignored or illicit ##ly excluded from the discussion c though the only logical way to remedy the b ##lat ##ant im ##balance in the above section is to include quotes by ##on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with imp ##uni ##ty by the ap ##ologists for marxist terrorism who have done their best to sabotage and wreck both the article and the discussion among the tactics deployed by the ap ##ologist wreck ##ers and sa ##bot ##eurs the following may be identified as representative examples a it is claimed that marx and eng ##els did not advocate terrorism despite the fact that scholarly works like the international encyclopedia of terrorism [SEP],[CLS] shelly shock shelly shock is [SEP],[CLS] i do not care refer to on ##g ten ##g che ##ong talk page is la go ##ut ##te de pl ##ui ##e writing a biography or writing the history of trade unions she is making use of the dead to push her agenda again right before elections too how timely [SEP]\n",
       "y: MultiCategoryList\n",
       "toxic,,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: TextList\n",
       "[CLS] gee ##z are you forget ##ful we have already discussed why marx was not an anarchist ie he wanted to use a state to mold his socialist man er ##go he is a stat ##ist the opposite of an anarchist i know a guy who says that when he gets old and his teeth fall out hell quit eating meat would you call him a vegetarian [SEP],[CLS] car ##io ##ca rf ##a thanks for your support on my request for ad ##mins ##hip the final outcome was so i am now an administrator if you have any comments or concerns on my actions as an administrator please let me know thank you [SEP],[CLS] birthday no worries it is what i do enjoy ur day ##talk ##e [SEP],[CLS] pseudo ##sc ##ience category i am assuming that this article is in the pseudo ##sc ##ience category because of it is association with creation ##ism however there are modern scientific ##ally ##ac ##ce ##pt ##ed variants of cat ##ast ##rop ##hism that have nothing to do with creation ##ism and they are even mentioned in the article i think the connection to pseudo ##sc ##ience needs to be clarified or the article made more general and less creation ##isms ##pe ##ci ##fi ##c and the category tag removed entirely [SEP],[CLS] and if such phrase exists it would be provided by search engine even if mentioned page is not available as a whole [SEP]\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7fb2a2691d90>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (3): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Dropout(p=0.1)\n",
       "  (1): Linear(in_features=768, out_features=6, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.save('head-2')\n",
    "learner.load('head-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 6.31E-07\n",
      "Min loss divided by 10: 1.74E-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.unfreeze()\n",
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.045203</td>\n",
       "      <td>0.044993</td>\n",
       "      <td>0.980335</td>\n",
       "      <td>35:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.037312</td>\n",
       "      <td>0.043990</td>\n",
       "      <td>0.980684</td>\n",
       "      <td>32:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(2, slice(5e-6, 5e-5), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: TextList\n",
       "[CLS] grandma terri should burn in trash grandma terri is trash i hate grandma terri fk her to hell [SEP],[CLS] may utc it would be easiest if you were to admit to being a member of the involved portuguese lodge and then there would be no requirement to acknowledge whether you had a previous account carlos bot ##el ##ho did not have a good record or not and i would then remove the sock ##pu ##ppet template as irrelevant w ##pc ##oi permit is people to edit those articles such as ms ##ja ##pan does but just means you have to be more careful in ensuring that references back your edit is and that np ##ov is upheld [SEP],[CLS] the object ##ivity of this discussion is doubtful none ##xi ##sten ##t as indicated earlier the section on marxist leaders views is misleading a it lays un ##war ##rant ##ed and excessive emphasis on tr ##ots ##ky creating the misleading impression that other prominent marxist ##s marx eng ##els lenin did not advocate and ##or practiced terrorism b it lays un ##war ##rant ##ed and excessive emphasis on the theoretical rejection of individual terrorism creating the misleading impression that this is the main only marxist position on terrorism the discussion is not being properly monitored a no disc ##ern ##ible attempt is being made to establish and maintain an acceptable degree of object ##ivity b important and relevant scholarly works such as the international encyclopedia of terrorism are being ignored or illicit ##ly excluded from the discussion c though the only logical way to remedy the b ##lat ##ant im ##balance in the above section is to include quotes by ##on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with imp ##uni ##ty by the ap ##ologists for marxist terrorism who have done their best to sabotage and wreck both the article and the discussion among the tactics deployed by the ap ##ologist wreck ##ers and sa ##bot ##eurs the following may be identified as representative examples a it is claimed that marx and eng ##els did not advocate terrorism despite the fact that scholarly works like the international encyclopedia of terrorism [SEP],[CLS] shelly shock shelly shock is [SEP],[CLS] i do not care refer to on ##g ten ##g che ##ong talk page is la go ##ut ##te de pl ##ui ##e writing a biography or writing the history of trade unions she is making use of the dead to push her agenda again right before elections too how timely [SEP]\n",
       "y: MultiCategoryList\n",
       "toxic,,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: TextList\n",
       "[CLS] gee ##z are you forget ##ful we have already discussed why marx was not an anarchist ie he wanted to use a state to mold his socialist man er ##go he is a stat ##ist the opposite of an anarchist i know a guy who says that when he gets old and his teeth fall out hell quit eating meat would you call him a vegetarian [SEP],[CLS] car ##io ##ca rf ##a thanks for your support on my request for ad ##mins ##hip the final outcome was so i am now an administrator if you have any comments or concerns on my actions as an administrator please let me know thank you [SEP],[CLS] birthday no worries it is what i do enjoy ur day ##talk ##e [SEP],[CLS] pseudo ##sc ##ience category i am assuming that this article is in the pseudo ##sc ##ience category because of it is association with creation ##ism however there are modern scientific ##ally ##ac ##ce ##pt ##ed variants of cat ##ast ##rop ##hism that have nothing to do with creation ##ism and they are even mentioned in the article i think the connection to pseudo ##sc ##ience needs to be clarified or the article made more general and less creation ##isms ##pe ##ci ##fi ##c and the category tag removed entirely [SEP],[CLS] and if such phrase exists it would be provided by search engine even if mentioned page is not available as a whole [SEP]\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7fb2a2691d90>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model/', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (3): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Dropout(p=0.1)\n",
       "  (1): Linear(in_features=768, out_features=6, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.save('head-3')\n",
    "learner.load('head-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MultiCategory ,\n",
       " tensor([0., 0., 0., 0., 0., 0.]),\n",
       " tensor([4.0619e-02, 2.0407e-05, 2.1858e-03, 4.0995e-04, 3.2507e-03, 1.0345e-03]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'you are so sweet'\n",
    "learner.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MultiCategory toxic;obscene;insult,\n",
       " tensor([1., 0., 1., 0., 1., 0.]),\n",
       " tensor([0.9939, 0.2122, 0.9616, 0.0284, 0.8392, 0.1226]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'you are pathetic piece of shit'\n",
    "learner.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
